import os
import io
import time
import webrtcvad
import numpy as np
import soundfile as sf
from pydub import AudioSegment
from pyannote.audio import Pipeline
from faster_whisper import WhisperModel

# åˆå§‹åŒ–èªè€…åˆ†é›¢ pipelineï¼ˆéœ€ HuggingFace Tokenï¼‰
HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN") or "hf_XXXX"  # <<-- æ›¿æ›ç‚ºä½ çš„ token
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=HUGGINGFACE_TOKEN)

# åˆå§‹åŒ– Whisper æ¨¡å‹ï¼ˆæ”¯æ´ä¸­æ–‡ï¼‰
whisper_model = WhisperModel("medium", device="cuda", compute_type="float16")

# åˆå§‹åŒ– VADï¼ˆWebRTCï¼‰
vad = webrtcvad.Vad()
vad.set_mode(2)  # 0: aggressive, 3: very aggressive

# æ¨¡æ“¬è¼¸å…¥éŸ³æª”åˆ‡æˆæ¯ 200ms ä¸Ÿé€² bufferï¼ˆå¯æ”¹ç‚ºä½ çš„ä¸²æµä¾†æºï¼‰
AUDIO_PATH = "sample_zh.wav"  # ä½ çš„ä¸­æ–‡èªéŸ³æª”
original = AudioSegment.from_wav(AUDIO_PATH)
frame_ms = 200
frame_bytes = int(original.frame_rate * (frame_ms / 1000.0) * 2)

buffer = []
silent_counter = 0
trigger_silence_threshold = 5

print("ğŸ” é–‹å§‹æ¨¡æ“¬ä¸²æµè¼¸å…¥...")

for i in range(0, len(original), frame_ms):
    chunk = original[i:i+frame_ms]
    raw_audio = chunk.raw_data
    is_speech = vad.is_speech(raw_audio, sample_rate=original.frame_rate)

    if is_speech:
        buffer.append(chunk)
        silent_counter = 0
    else:
        silent_counter += 1
        if buffer:
            buffer.append(chunk)

    # å¦‚æœé€£çºŒéœéŸ³æ®µè¶…éé–€æª»ï¼Œè§¸ç™¼èªéŸ³æ¨è«–
    if silent_counter >= trigger_silence_threshold and buffer:
        combined = sum(buffer)
        wav_io = io.BytesIO()
        combined.export(wav_io, format="wav")
        wav_io.seek(0)
        audio_data, _ = sf.read(wav_io)

        # å„²å­˜æš«å­˜æª”ä¾› pyannote ä½¿ç”¨
        with open("temp_stream.wav", "wb") as f:
            combined.export(f, format="wav")

        print("ğŸ§  é–‹å§‹èªè€…åˆ†é›¢èˆ‡è¾¨è­˜...")

        diarization = diarization_pipeline("temp_stream.wav")

        for turn, _, speaker in diarization.itertracks(yield_label=True):
            seg = combined[turn.start * 1000: turn.end * 1000]
            seg_io = io.BytesIO()
            seg.export(seg_io, format="wav")
            seg_io.seek(0)
            audio_clip, _ = sf.read(seg_io)

            segments, _ = whisper_model.transcribe(audio_clip, language="zh", vad_filter=False)
            full_text = " ".join([seg.text for seg in segments])
            print(f"ğŸ—£ï¸ {speaker}: {full_text}")

        # æ¸…ç©º buffer
        buffer = []
        silent_counter = 0

    time.sleep(0.05)  # æ¨¡æ“¬ä¸²æµé–“éš”

print("âœ… æ¨¡æ“¬çµæŸ")
