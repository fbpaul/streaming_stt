import os
import numpy as np
import webrtcvad
import torchaudio
import torch
from pyannote.audio import Pipeline
# from faster_whisper import WhisperModel
import time

# -------------------
# åƒæ•¸è¨­å®š
# -------------------
CHUNK_DURATION = 0.5  # æ¯æ¬¡ä¸Ÿ 0.5 ç§’æ¨¡æ“¬ä¸²æµ
SAMPLE_RATE = 16000
VAD_MODE = 3  # æœ€åš´æ ¼èªéŸ³åµæ¸¬
LANGUAGE = "zh"

# -------------------
# åˆå§‹åŒ–æ¨¡å‹
# -------------------
asr_model = WhisperModel("medium", device="cuda", compute_type="float16")
vad = webrtcvad.Vad(VAD_MODE)
diarization_pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization",
                                                use_auth_token=os.getenv("HUGGINGFACE_TOKEN"))

# -------------------
# è¼‰å…¥èˆ‡åˆ‡å‰²éŸ³è¨Š
# -------------------
def load_audio_and_chunk(wav_path):
    waveform, sr = torchaudio.load(wav_path)
    if sr != SAMPLE_RATE:
        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)
    audio = waveform[0].numpy()

    chunk_size = int(CHUNK_DURATION * SAMPLE_RATE)
    return [audio[i:i + chunk_size] for i in range(0, len(audio), chunk_size)]

# -------------------
# åˆ¤æ–·æ˜¯å¦ç‚ºèªéŸ³
# -------------------
def is_speech(chunk):
    pcm = (chunk * 32768).astype(np.int16).tobytes()
    return vad.is_speech(pcm, SAMPLE_RATE)

# -------------------
# æ¨¡æ“¬å³æ™‚èªéŸ³è½‰æ›æµç¨‹
# -------------------
def transcribe_streaming(chunks):
    buffer = []
    sentence_id = 0

    for i, chunk in enumerate(chunks):
        if is_speech(chunk):
            buffer.append(chunk)
        else:
            # å¦‚æœç´¯ç©çš„èªéŸ³é•·åº¦è¶…é1.5ç§’ï¼Œå‰‡é€²è¡Œè½‰æ›
            if len(buffer) * CHUNK_DURATION >= 1.5:
                audio_segment = np.concatenate(buffer)
                segments, _ = asr_model.transcribe(audio_segment, language=LANGUAGE)

                print(f"\nğŸ—¨ ç¬¬ {sentence_id + 1} å¥è½‰è­¯çµæœï¼š")
                for seg in segments:
                    print(f"{seg.text.strip()}")

                buffer = []
                sentence_id += 1
            else:
                buffer = []

# -------------------
# èªè€…åˆ†é›¢
# -------------------
def speaker_diarization(wav_path):
    result = diarization_pipeline(wav_path)
    print("\nğŸ§‘â€ğŸ¤â€ğŸ§‘ èªè€…åˆ†é›¢çµæœ:")
    for turn, _, speaker in result.itertracks(yield_label=True):
        print(f"{turn.start:.1f}s - {turn.end:.1f}s: {speaker}")

# -------------------
# ä¸»ç¨‹å¼
# -------------------
def main():
    wav_path = "example.wav"  # æ”¹æˆä½ çš„æª”å
    chunks = load_audio_and_chunk(wav_path)

    print("ğŸ”Š é–‹å§‹æ¨¡æ“¬ä¸²æµèªéŸ³è¾¨è­˜...\n")
    transcribe_streaming(chunks)
    speaker_diarization(wav_path)

if __name__ == "__main__":
    main()
