import torchaudio
import time
import sounddevice as sd
import sys
from config import SAMPLE_RATE, CHUNK_DURATION, vad
from vad import split_audio_to_chunks
from transcriber_openai import StreamingTranscriber

def main():
    wav_path = "./shorts.wav"
    waveform, sr = torchaudio.load(wav_path)
    if sr != SAMPLE_RATE:
        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)

    audio = waveform[0].numpy()
    chunks = split_audio_to_chunks(audio, SAMPLE_RATE, CHUNK_DURATION)

    transcriber = StreamingTranscriber(vad)

    print("ğŸ”Š é–‹å§‹æ’­æ”¾èˆ‡å³æ™‚è¾¨è­˜...\n")

    interim_text = ""
    interim_active = False

    for chunk in chunks:
        # æ’­æ”¾é€™æ®µéŸ³è¨Š
        sd.play(chunk, samplerate=SAMPLE_RATE)
        
        # ä¸¦è¡Œè¾¨è­˜
        result = transcriber.process_chunk(chunk)

        if result:
            if result["type"] == "interim":
                interim_text += result["text"]
                sys.stdout.write("\r")
                sys.stdout.write(f"â³ [æš«å®š] {interim_text[:80]}")
                sys.stdout.flush()
                interim_active = True

            elif result["type"] == "final":
                if interim_active:
                    sys.stdout.write("\r" + " " * 100 + "\r")
                print(f"âœ… [ä¿®æ­£] ({result['speaker']}) {result['start']}~{result['end']}s: {result['text']}")
                interim_text = ""
                interim_active = False

        # ç­‰å¾…æ’­æ”¾å®Œç•¢ï¼ˆéå¼·åˆ¶ç­‰ï¼Œä½†è¼ƒç©©å®šï¼‰
        sd.wait()

if __name__ == "__main__":
    main()
